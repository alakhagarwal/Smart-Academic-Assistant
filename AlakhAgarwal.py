import streamlit as st
from langchain_groq import ChatGroq
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import requests
from PyPDF2 import PdfReader
from langchain_community.document_loaders import PyPDFLoader
from langchain_core import load
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv
from pydantic  import BaseModel,Field
from langchain_core.output_parsers import PydanticOutputParser,StrOutputParser
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langchain import hub #for generating pre defined prompt

load_dotenv()

if "final_summary" not in st.session_state:
    st.session_state.final_summary = None

# we are using summary  to generate mcq and explanation bcoz otherwise it will exceed llm imput token limit
# We are storing it in session state so that we dont need to generate it again and again as it will take time

def chunk_text(text, max_chars=10000):   #The function chunk_text breaks a big text into smaller parts so bcoz the model cannot process large text at once
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunks.append(text[start:end])
        start = end
    return chunks                 #It returns a list of that divided text

# Function to extract text from PDFs
def get_pdf_text(pdf_docs):    
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def generate_summary_from_chunks():
    text = get_pdf_text(uploaded_files)
    chunks = chunk_text(text, max_chars=10000) #list of text divided into small chunks
    results = []
    for chunk in chunks:
        output = get_summary.invoke({"input": f"Find the summary of the following text \n{chunk}"}) #we call agent for each chunk and stores it's summary inside a list
        results.append(output["output"])
    final_summary = "\n".join(results)  #combine all the summary of chunks
    st.session_state.final_summary = final_summary #storing it in session state for further reuse(like in generating mcqs or topic wise explanation)
    return final_summary


class QAResponse(BaseModel):        # Defined a Basemodel Class for Structured output
    question: str = Field(description="The academic question asked by the user")
    answer: str = Field(description="The answer generated by the LLM")
    source_document: str = Field(description="Name or description of the source document")
    confidence_score: float = Field(description="Confidence score of the answer")


parser = PydanticOutputParser(pydantic_object=QAResponse)

str_parser = StrOutputParser()

llm=ChatGroq(model="llama3-8b-8192")

prompt_for_summary = PromptTemplate(
    template="Generate a detailed Summary on the following text \n {text}",
    input_variables=["text"]
)

summary_chain = prompt_for_summary|llm|str_parser

@tool
def get_summary(text : str) ->str:            #Created a tool for getting summary
    """Generates Summary of the given text"""
    return summary_chain.invoke({"text":text})

prompt_for_mcqs = PromptTemplate(
    template=(
        "Based on the following academic content, first create a concise summary, "
        "then generate 5 multiple choice questions (MCQs) from it.\n\n"
        "Text:\n{text}\n\n"
        "Format:\n1. Question?\n    A. Option 1\n    B. Option 2\n    C. Option 3\n    D. Option 4\nAnswer: X"
    ),
    input_variables=["text"]
)

mcq_chain = prompt_for_mcqs|llm|str_parser

@tool                                               #Created a tool for getting mcqs
def get_mcqs(text : str) ->str:
    """Generates 5 most important mcqs by generating the summary of the given text"""
    return mcq_chain.invoke({"text":text})

prompt_for_topic_wise = PromptTemplate(
    template= ("Based on the following academic content, identify key topics and explain each one clearly "
        "with relevant examples if possible.\n\n"
        "Content:\n{text}\n\n"
        "Format:\n\n"
        "1. Topic Name:\nExplanation...\nExample (if applicable)...\n"),
    input_variables=["text"]
)

topic_chain = prompt_for_topic_wise|llm|str_parser

@tool
def get_topic_wise_explanation(text : str) ->str:                          #Created a tool for getting topic wise explanation
    """Generates topic wise explanation of the following text"""
    return topic_chain.invoke({"text":text})


def  get_pdf_docs(pdf_docs): #we will return a list of documents objects so that we can also preserve the meta data for the structured output
    docs = []
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)     #Earlier i was returning raw text only but the problem was that we were required to send some metadata also as context to llm so that it can tell us about the source in the output 
        filename = pdf.name  # Get file name             #Learnt these commands from github codes as they were not mentioned by the tutor nor by the documentations anywhere
        for i, page in enumerate(pdf_reader.pages):
            text = page.extract_text()
            if text:
                docs.append(Document(
                page_content=text,
                metadata={"source": filename, "page": i + 1}
            ))
    return docs
# -------------------- Page Configuration --------------------
st.set_page_config(page_title="Smart Academic Assistant", layout="centered")

# -------------------- Title --------------------
st.title("ðŸ“š Smart Academic Assistant")
st.write("Upload your academic documents and ask questions to get structured answers.")

# -------------------- File Upload Section --------------------
uploaded_files = st.file_uploader(                 #I was not abel to figure out any document loaders which can load the text and docx file without loading it in any other database
    "Upload academic documents PDF:",              #So unfortunately i had to limit the project only till Multiple pdf documents
    type=["pdf"],
    accept_multiple_files=True
)

# -------------------- Question Input --------------------
question = st.text_input("Enter your academic question:")

# -------------------- Submit Button --------------------
if st.button("Get Answer"):
    if not uploaded_files or not question:
        st.warning("Please upload at least one document and enter a question.")
    else:
        extracted_docs = get_pdf_docs(uploaded_files)  #gets a list of document objects
        
        # 2. Split documents using RecursiveCharacterTextSplitter or similar

        splitter = RecursiveCharacterTextSplitter(
        chunk_size = 500,
        chunk_overlap = 50,
        )

        chunks = split_docs = splitter.split_documents(extracted_docs)      #splitting the docs into smaller chunks
        #it return a list of document objects
        #          

        # 3. Create embeddings and store in vector store (e.g., FAISS, Chroma)
        embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_store = FAISS.from_documents(chunks, embedding_model)     #stored in vector store for retrieval process


        # 4. Retrieve relevant chunks based on the question

        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})    #I tried setting different values of k but 4 gave good accuracy in my case
        retrieved_docs    = retriever.invoke(question) #gets four most relevant documents

        # 5. Use Groq-hosted LLM via LangChain (e.g., Mixtral, Gemma, Llama3)
        
        # Model declared above 

        prompt = PromptTemplate(
        template="""
        You are a helpful assistant.
        Answer ONLY from the provided context.
        Read the full context thoroughly and answer in detail if required.
        If the context is insufficient, just say you don't know.

        Context: {context}
        Question: {question}

        Format your response as follows:
        {format_instructions}
        """,
        input_variables = ['context', 'question'],
        partial_variables={"format_instructions":parser.get_format_instructions()} 
        )

        
        # 6. Use Output Parser to format structured response

        chain = prompt|llm

        result = chain.invoke({"question":question,"context":retrieved_docs})  #returns the output


        # Example output format (replace this with actual output):
        # response = {
        #     "question": question,
        #     "answer": "Your answer here",
        #     "source_document": "Document Name",
        #     "confidence_score": "0.93"
        # }
        #
        # st.subheader("ðŸ“„ Answer:")
        # st.json(response)

    st.info(result.content)   #prints the output in json format

# -------------------- Bonus Section: Agent Tools --------------------
st.markdown("---")
st.subheader("ðŸ§  Bonus Tools ( Optional )")
prompt = hub.pull("hwchase17/react")

col1, col2, col3 = st.columns(3)

# SO BASICALLY I TRIED DOING THE BONUS CONTENT WITH AN (AGENT+AGENT EXECUTIONER) BUT THE PROBLEM WAS THAT  
# DUE TO ADOPTING THE BEHAVIOUR OF AN REACT AGENT IT WAS EXECUTING THE ALL THREE STEPS AT ONCE(SUMMARY,MCQ,EXPLANATION)
# WHICH WAS TAKING TOO MUCH TIME.IT WAS KINDA GOING INTO AN INFINITE CHAIN OF THOUGHT ACTION WHICH WAS TAKING THE STEP TOO SLOW
# SO MAYBE IT WAS NOT ABLE TO HANDLE A SINGLE TOOL CALL SO I HAD TO REMOVE IT
# I ALSO TRIED PASSING A CUSTOM PROMPR TO THE AGENT BUT STILL IT DIDNT WORK
# SO AT LAST I DECIDED TO JUST MAKE THE TOOLS AND DIRECTLY INVOKE THE TOOLS INSTEAD OF IT BEING DONE BY AN AGENT


with col1:
    if st.button("Summarize Document"):
        text = get_pdf_text(uploaded_files)     #Gets the raw text from the list of docoument objects as we want to generate summary form them and dont require metadata
        chunks = chunk_text(text, max_chars=10000)  # Split text if too large

        results = []
        for chunk in chunks:
           output = get_summary.invoke({"text": chunk})       #SO IF I DIRECTLY PASSED THE WHOLE TEXT TO THE FUCNTION THEN IT WOULD exceed the toke input limit of llm
           results.append(output)                             #SO I AM GENERATING THE SUMMARY OF EACH CHUNK AND THEN JOINI GTHEM ALL TO GET THE FINAL OUPUT

        final_summary = "\n".join(results)
        st.session_state.final_summary = final_summary  # Store for reuse
        st.info(final_summary)

with col2:
    if st.button("Generate MCQs"):                               #FOR GENERATING MCQS FROM THE SUMMARY ITSELF OTHERWISE IT WILL AGAIN exceed the toke input limit of llm
        if not st.session_state.get("final_summary"):
            generate_summary_from_chunks()
        
        output = get_mcqs.invoke({"text": st.session_state.final_summary})
        st.info(output)

with col3:
    if st.button("Topic-wise Explanation"):
        if not st.session_state.get("final_summary"):      #FOR GENERATING EXPLANATION TOPIC WISE FROM THE SUMMARY ITSELF OTHERWISE IT WILL AGAIN exceed the toke input limit of llm
            generate_summary_from_chunks()               
                                                          # we are storing it in session state so that we dont need to generate it again and again as it wil take time
      
        output = get_topic_wise_explanation.invoke({"text": st.session_state.final_summary})
        st.info(output)


# -------------------- Footer --------------------
st.markdown("---")
st.caption("Mentox Bootcamp Â· Final Capstone Project Â· Phase 1")
