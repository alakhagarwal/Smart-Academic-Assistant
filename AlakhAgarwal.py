import streamlit as st
from langchain_groq import ChatGroq
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import requests
from PyPDF2 import PdfReader
from langchain_community.document_loaders import PyPDFLoader
from langchain_core import load
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv
from pydantic  import BaseModel,Field
from langchain_core.output_parsers import PydanticOutputParser,StrOutputParser
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langchain.agents import create_react_agent, AgentExecutor
from langchain import hub #for generating pre defined prompt

load_dotenv()

if "final_summary" not in st.session_state:
    st.session_state.final_summary = None

#we are using summary  to generate mcq and explanation bcoz otherwise it will exceed llm imput token limit
# we are storing it in session state so that we dont need to generate it again and again as it wil take time

# Helper function to generate summary via agent_executor

def chunk_text(text, max_chars=10000):   #The function chunk_text breaks a big text into smaller parts so bcoz the model cannot process large text at once
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_chars
        chunks.append(text[start:end])
        start = end
    return chunks

def generate_summary_from_chunks():
    text = get_pdf_text(uploaded_files)
    chunks = chunk_text(text, max_chars=10000) #list of text divided into small chunks
    results = []
    for chunk in chunks:
        output = get_summary.invoke({"input": f"Find the summary of the following text \n{chunk}"}) #we call agent for each chunk
        results.append(output["output"])
    final_summary = "\n".join(results)  #combine all the summary of chunks
    st.session_state.final_summary = final_summary #storing it in session state for further reuse
    return final_summary




class QAResponse(BaseModel):
    question: str = Field(description="The academic question asked by the user")
    answer: str = Field(description="The answer generated by the LLM")
    source_document: str = Field(description="Name or description of the source document")
    confidence_score: float = Field(description="Confidence score of the answer")


parser = PydanticOutputParser(pydantic_object=QAResponse)

str_parser = StrOutputParser()

llm=ChatGroq(model="llama3-8b-8192")

prompt_for_summary = PromptTemplate(
    template="Generate a detailed Summary on the following text \n {text}",
    input_variables=["text"]
)

summary_chain = prompt_for_summary|llm|str_parser

@tool
def get_summary(text : str) ->str:
    """Generates Summary of the given text"""
    return summary_chain.invoke({"text":text})

# Function to extract text from PDFs
def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

prompt_for_mcqs = PromptTemplate(
    template=(
        "Based on the following academic content, first create a concise summary, "
        "then generate 5 multiple choice questions (MCQs) from it.\n\n"
        "Text:\n{text}\n\n"
        "Format:\n1. Question?\n    A. Option 1\n    B. Option 2\n    C. Option 3\n    D. Option 4\nAnswer: X"
    ),
    input_variables=["text"]
)

mcq_chain = prompt_for_mcqs|llm|str_parser

@tool
def get_mcqs(text : str) ->str:
    """Generates 5 most important mcqs by generating the summary of the given text"""
    return mcq_chain.invoke({"text":text})

prompt_for_topic_wise = PromptTemplate(
    template= ("Based on the following academic content, identify key topics and explain each one clearly "
        "with relevant examples if possible.\n\n"
        "Content:\n{text}\n\n"
        "Format:\n\n"
        "1. Topic Name:\nExplanation...\nExample (if applicable)...\n"),
    input_variables=["text"]
)

topic_chain = prompt_for_topic_wise|llm|str_parser

@tool
def get_topic_wise_explanation(text : str) ->str:
    """Generates topic wise explanation of the following text"""
    return topic_chain.invoke({"text":text})


def  get_pdf_docs(pdf_docs): #we will return a list of documents objects so that we can also preserve the meta data for the strcutured output
    docs = []
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        filename = pdf.name  # Get file name
        for i, page in enumerate(pdf_reader.pages):
            text = page.extract_text()
            if text:
                docs.append(Document(
                page_content=text,
                metadata={"source": filename, "page": i + 1}
            ))
    return docs
# -------------------- Page Configuration --------------------
st.set_page_config(page_title="Smart Academic Assistant", layout="centered")

# -------------------- Title --------------------
st.title("ðŸ“š Smart Academic Assistant")
st.write("Upload your academic documents and ask questions to get structured answers.")

# -------------------- File Upload Section --------------------
uploaded_files = st.file_uploader(                 #considering the user uploads only one file
    "Upload academic documents PDF:",
    type=["pdf"],
    accept_multiple_files=True
)

# -------------------- Question Input --------------------
question = st.text_input("Enter your academic question:")

# -------------------- Submit Button --------------------
if st.button("Get Answer"):
    if not uploaded_files or not question:
        st.warning("Please upload at least one document and enter a question.")
    else:
        extracted_docs = get_pdf_docs(uploaded_files)  
        
        # 2. Split documents using RecursiveCharacterTextSplitter or similar

        splitter = RecursiveCharacterTextSplitter(
        chunk_size = 500,
        chunk_overlap = 50,
        )

        chunks = split_docs = splitter.split_documents(extracted_docs)
        #it return a list of document objects

        # Input: A list of strings (usually 1 or more documents)

        # Output: A list of Document objects:
         

        # 3. Create embeddings and store in vector store (e.g., FAISS, Chroma)
        embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_store = FAISS.from_documents(chunks, embedding_model)


        # 4. Retrieve relevant chunks based on the question

        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})
        retrieved_docs    = retriever.invoke(question) #gets three most relevant documents

        # 5. Use Groq-hosted LLM via LangChain (e.g., Mixtral, Gemma, Llama3)
        # Model declared above 

        prompt = PromptTemplate(
        template="""
        You are a helpful assistant.
        Answer ONLY from the provided context.
        Read the full context thoroughly and answer in detail if required.
        If the context is insufficient, just say you don't know.

        Context: {context}
        Question: {question}

        Format your response as follows:
        {format_instructions}
        """,
        input_variables = ['context', 'question'],
        partial_variables={"format_instructions":parser.get_format_instructions()} 
        )

        
        # 6. Use Output Parser to format structured response

        chain = prompt|llm

        result = chain.invoke({"question":question,"context":retrieved_docs})


        # Example output format (replace this with actual output):
        # response = {
        #     "question": question,
        #     "answer": "Your answer here",
        #     "source_document": "Document Name",
        #     "confidence_score": "0.93"
        # }
        #
        # st.subheader("ðŸ“„ Answer:")
        # st.json(response)

    st.info(result.content)

# -------------------- Bonus Section: Agent Tools --------------------
st.markdown("---")
st.subheader("ðŸ§  Bonus Tools ( Optional )")
prompt = hub.pull("hwchase17/react")

col1, col2, col3 = st.columns(3)

with col1:
    if st.button("Summarize Document"):
        text = get_pdf_text(uploaded_files)
        chunks = chunk_text(text, max_chars=10000)  # Split text if too large

        results = []
        for chunk in chunks:
           output = get_summary.invoke({"text": chunk})
           results.append(output)

        final_summary = "\n".join(results)
        st.session_state.final_summary = final_summary  # Store for reuse
        st.info(final_summary)

with col2:
    if st.button("Generate MCQs"):
        if not st.session_state.get("final_summary"):
            generate_summary_from_chunks()
        
        output = get_mcqs.invoke({"text": st.session_state.final_summary})
        st.info(output)

with col3:
    if st.button("Topic-wise Explanation"):
        if not st.session_state.get("final_summary"):
            generate_summary_from_chunks()                #we are using summary  to generate mcq and explanation bcoz otherwise it will exceed llm imput token limit
                                                          # we are storing it in session state so that we dont need to generate it again and again as it wil take time
        input = f"Use only the `get_topic_wise_explanation` tool to explain the following text by topic:\n\n{st.session_state.final_summary}"
        output = get_topic_wise_explanation.invoke({"text": st.session_state.final_summary})
        st.info(output)


# -------------------- Footer --------------------
st.markdown("---")
st.caption("Mentox Bootcamp Â· Final Capstone Project Â· Phase 1")
