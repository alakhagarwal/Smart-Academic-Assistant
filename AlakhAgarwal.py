import streamlit as st
from langchain_groq import ChatGroq
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import requests
from PyPDF2 import PdfReader
from langchain_community.document_loaders import PyPDFLoader
from langchain_core import load
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from dotenv import load_dotenv
from pydantic  import BaseModel,Field
from langchain_core.output_parsers import PydanticOutputParser

load_dotenv()

from pydantic import BaseModel, Field

class QAResponse(BaseModel):
    question: str = Field(description="The academic question asked by the user")
    answer: str = Field(description="The answer generated by the LLM")
    source_document: str = Field(description="Name or description of the source document")
    confidence_score: float = Field(description="Confidence score of the answer")


parser = PydanticOutputParser(pydantic_object=QAResponse)


def  get_pdf_text(pdf_docs): #we will return a list of documents objects so that we can also preserve the meta data for the strcutured output
    docs = []
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        filename = pdf.name  # Get file name
        for i, page in enumerate(pdf_reader.pages):
            text = page.extract_text()
            if text:
                docs.append(Document(
                page_content=text,
                metadata={"source": filename, "page": i + 1}
            ))
    return docs
# -------------------- Page Configuration --------------------
st.set_page_config(page_title="Smart Academic Assistant", layout="centered")

# -------------------- Title --------------------
st.title("ðŸ“š Smart Academic Assistant")
st.write("Upload your academic documents and ask questions to get structured answers.")

# -------------------- File Upload Section --------------------
uploaded_files = st.file_uploader(                 #considering the user uploads only one file
    "Upload academic documents PDF:",
    type=["pdf"],
    accept_multiple_files=True
)

# -------------------- Question Input --------------------
question = st.text_input("Enter your academic question:")

# -------------------- Submit Button --------------------
if st.button("Get Answer"):
    if not uploaded_files or not question:
        st.warning("Please upload at least one document and enter a question.")
    else:
        extracted_docs = get_pdf_text(uploaded_files)  
        
        # 2. Split documents using RecursiveCharacterTextSplitter or similar

        splitter = RecursiveCharacterTextSplitter(
        chunk_size = 500,
        chunk_overlap = 50,
        )

        chunks = split_docs = splitter.split_documents(extracted_docs)
        #it return a list of document objects

        # Input: A list of strings (usually 1 or more documents)

        # Output: A list of Document objects:
         

        # 3. Create embeddings and store in vector store (e.g., FAISS, Chroma)
        embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_store = FAISS.from_documents(chunks, embedding_model)


        # 4. Retrieve relevant chunks based on the question

        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        retrieved_docs    = retriever.invoke(question) #gets three most relevant documents

        # 5. Use Groq-hosted LLM via LangChain (e.g., Mixtral, Gemma, Llama3)
        llm=ChatGroq(model="llama-3.3-70b-versatile")

        prompt = PromptTemplate(
        template="""
        You are a helpful assistant.
        Answer ONLY from the provided transcript context.
        If the context is insufficient, just say you don't know.

        Context: {context}
        Question: {question}

        Format your response as follows:
        {format_instructions}
        """,
        input_variables = ['context', 'question'],
        partial_variables={"format_instructions":parser.get_format_instructions()} 
        )

        
        # 6. Use Output Parser to format structured response

        chain = prompt|llm

        result = chain.invoke({"question":question,"context":retrieved_docs})



        # Example output format (replace this with actual output):
        # response = {
        #     "question": question,
        #     "answer": "Your answer here",
        #     "source_document": "Document Name",
        #     "confidence_score": "0.93"
        # }
        #
        # st.subheader("ðŸ“„ Answer:")
        # st.json(response)

    st.info(result.content)

# -------------------- Bonus Section: Agent Tools --------------------
st.markdown("---")
st.subheader("ðŸ§  Bonus Tools ( Optional )")

col1, col2, col3 = st.columns(3)

with col1:
    if st.button("Summarize Document"):
        # TODO: Implement SummarizeDocumentTool using LangChain agent
        st.info("Summary will be shown here.")

with col2:
    if st.button("Generate MCQs"):
        # TODO: Implement GenerateMCQsTool using LangChain agent
        st.info("Generated MCQs will appear here.")

with col3:
    if st.button("Topic-wise Explanation"):
        # TODO: Implement TopicWiseExplanationTool using LangChain agent
        st.info("Topic-wise explanation will be displayed here.")

# -------------------- Footer --------------------
st.markdown("---")
st.caption("Mentox Bootcamp Â· Final Capstone Project Â· Phase 1")
