import streamlit as st
from langchain_groq import ChatGroq
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage
import requests
from PyPDF2 import PdfReader
from langchain_community.document_loaders import PyPDFLoader
from langchain_core import load
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from pydantic  import BaseModel,Field
from langchain_core.output_parsers import StrOutputParser
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langchain import hub
from PIL import Image

st.set_page_config(page_title="Your Study-Buddy", layout="centered")

st.sidebar.image("logo3.png",width=200)

if "final_summary" not in st.session_state:
    st.session_state.final_summary = None

def chunk_text(text, max_chars=10000): 
    start = 0
    chunks=[]
    while start < len(text):
        end = start + max_chars
        chunks.append(text[start:end])
        start = end
    return chunks               

def get_pdf_text(pdf_docs):    
    text = ""
    for pdf in pdf_docs:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

def generate_summary_from_chunks():
    text = get_pdf_text(uploaded_files)
    chunks = chunk_text(text, max_chars=10000) 
    results = []
    for chunk in chunks:
        output = get_summary.invoke({"input": f"Find the summary of the following text \n{chunk}"})
        results.append(output["output"])
    final_summary = "\n".join(results)  
    st.session_state.final_summary = final_summary 
    return final_summary


class QAResponse(BaseModel):        
    question: str = Field(description="The academic question asked by the user")
    answer: str = Field(description="The answer generated by the LLM")
    source_document: str = Field(description="Name or description of the source document")
    confidence_score: float = Field(description="Confidence score of the answer")


str_parser = StrOutputParser()

mod = st.sidebar.selectbox("Choose Model",["llama-3.3-70b-versatile","gemma2-9b-it","llama-3.1-8b-instant","meta-llama/llama-guard-4-12b"])


temp = st.sidebar.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.05)

tokens = st.sidebar.slider("Max Tokens",100,1000,900)

llm=ChatGroq(model=mod,temperature=temp,max_tokens=tokens,api_key=st.secrets["GROQ"]["api_key"])

prompt_for_summary = PromptTemplate(
    template="Generate a detailed Summary on the following text \n {text}",
    input_variables=["text"]
)

summary_chain = prompt_for_summary|llm|str_parser

@tool
def get_summary(text : str) ->str:          
    """Generates Summary of the given text"""
    return summary_chain.invoke({"text":text})

prompt_for_mcqs = PromptTemplate(
    template=(
        "Based on the following academic content, first create a concise summary, "
        "then generate 5 multiple choice questions (MCQs) from it.\n\n"
        "Text:\n{text}\n\n"
        "Format:\n1. Question?\n    A. Option 1\n    B. Option 2\n    C. Option 3\n    D. Option 4\nAnswer: X"
    ),
    input_variables=["text"]
)

mcq_chain = prompt_for_mcqs|llm|str_parser

@tool                                              
def get_mcqs(text : str) ->str:
    """Generates 5 most important mcqs by generating the summary of the given text"""
    return mcq_chain.invoke({"text":text})

prompt_for_topic_wise = PromptTemplate(
    template= ("Based on the following academic content, identify key topics and explain each one clearly "
        "with relevant examples if possible.\n\n"
        "Content:\n{text}\n\n"
        "Format:\n\n"
        "1. Topic Name:\nExplanation...\nExample (if applicable)...\n"),
    input_variables=["text"]
)

topic_chain = prompt_for_topic_wise|llm|str_parser

@tool
def get_topic_wise_explanation(text : str) ->str:                          
    """Generates topic wise explanation of the following text"""
    return topic_chain.invoke({"text":text})


col1, col2 = st.columns([1, 5])

with col1:
    st.image("logo3.png", width=400)  # Adjust width if needed

with col2:
    st.title("Your Study-Buddy")
   



uploaded_files = st.file_uploader(               
    "Upload academic documents PDF:",           
    accept_multiple_files=True
)

question = st.text_input("Enter your academic question:")

if st.button("Get Answer"):
    if not uploaded_files or not question:
        st.warning("Please upload at least one document and enter a question.")
    else:
        extracted_text = get_pdf_text(uploaded_files)  
        
        

        splitter = RecursiveCharacterTextSplitter(
        chunk_size = 500,
        chunk_overlap = 50,
        )

        chunks=splitter.create_documents([extracted_text])           

      
        embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')
        vector_store = FAISS.from_documents(chunks, embedding_model)    

        retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})   
        retrieved_docs    = retriever.invoke(question) 

        prompt = PromptTemplate(
        template="""
        You are a helpful assistant.
        Answer ONLY from the provided context.
        Read the full context thoroughly and answer in detail if required.
        If the context is insufficient, just say you don't know.

        Context: {context}
        Question: {question}
        """,
        input_variables = ['context', 'question']      
        )

        chain = prompt|llm

        result = chain.invoke({"question":question,"context":retrieved_docs})  


    st.info(result.content)   

st.markdown("---")
st.markdown("### üß† Bonus Tools")
prompt = hub.pull("hwchase17/react")

col1, col2, col3 = st.columns(3)


with col1:
    if  st.button("üìÑ Summarize Document"):
        text = get_pdf_text(uploaded_files)     
        chunks = chunk_text(text, max_chars=10000) 

        results = []
        for chunk in chunks:
           output = get_summary.invoke({"text": chunk})       
           results.append(output)                            

        final_summary = "\n".join(results)
        st.session_state.final_summary = final_summary  
        st.info(final_summary)

with col2:
    if  st.button("üìù Generate MCQs"):                              
        if not st.session_state.get("final_summary"):
            generate_summary_from_chunks()
        
        output = get_mcqs.invoke({"text": st.session_state.final_summary})
        st.info(output)

with col3:
    if st.button("üìö Topic-wise Explanation"):
        if not st.session_state.get("final_summary"):      
            generate_summary_from_chunks()               
                                                        
      
        output = get_topic_wise_explanation.invoke({"text": st.session_state.final_summary})
        st.info(output)


st.markdown("---")

